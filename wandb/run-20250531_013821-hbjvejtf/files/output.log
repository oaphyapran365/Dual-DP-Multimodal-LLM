2025-05-31 01:38:23,540 [INFO] Start training
2025-05-31 01:38:52,432 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2025-05-31 01:38:52,433 [INFO] Loaded 3439 records for train split from the dataset.
batch sizes [[12]]
module.llama_proj.weight
module.llama_proj.bias
2025-05-31 01:38:52,445 [INFO] number of trainable parameters: 3149824
2025-05-31 01:38:52,445 [INFO] Start training epoch 0, 200 iters per inner epoch.
Train: data epoch: [0]  [  0/200]  eta: 0:32:45  lr: 0.000001  loss: 0.6218  time: 9.8274  data: 0.0000  max mem: 35680
Train: data epoch: [0]  [ 50/200]  eta: 0:02:19  lr: 0.000008  loss: 0.6483  time: 0.6250  data: 0.0000  max mem: 36248
Train: data epoch: [0]  [100/200]  eta: 0:01:17  lr: 0.000015  loss: 0.5817  time: 0.6296  data: 0.0000  max mem: 36248
Train: data epoch: [0]  [150/200]  eta: 0:00:37  lr: 0.000023  loss: 0.6264  time: 0.7340  data: 0.0000  max mem: 36248
Train: data epoch: [0]  [199/200]  eta: 0:00:00  lr: 0.000030  loss: 0.7147  time: 0.6196  data: 0.0000  max mem: 36248
Train: data epoch: [0] Total time: 0:02:22 (0.7120 s / it)
2025-05-31 01:41:14,836 [INFO] Averaged stats: lr: 0.0000  loss: 0.6880
2025-05-31 01:41:14,841 [INFO] No validation splits found.
2025-05-31 01:41:14,862 [INFO] Saving checkpoint at epoch 0 to /mnt/bst/hxu10/hxu10/Abdullahil-Oaphy/MiniGPT-4/outputs/20250531013/checkpoint_0.pth.
2025-05-31 01:41:14,953 [INFO] Start training
2025-05-31 01:41:14,974 [INFO] Start training epoch 1, 200 iters per inner epoch.
Train: data epoch: [1]  [  0/200]  eta: 0:02:06  lr: 0.000028  loss: 0.7642  time: 0.6317  data: 0.0000  max mem: 36248
Train: data epoch: [1]  [ 50/200]  eta: 0:01:35  lr: 0.000027  loss: 0.6169  time: 0.6347  data: 0.0000  max mem: 36248
Train: data epoch: [1]  [100/200]  eta: 0:01:05  lr: 0.000026  loss: 0.6596  time: 0.7406  data: 0.0000  max mem: 36248
Train: data epoch: [1]  [150/200]  eta: 0:00:32  lr: 0.000025  loss: 0.6352  time: 0.6180  data: 0.0000  max mem: 36248
Train: data epoch: [1]  [199/200]  eta: 0:00:00  lr: 0.000023  loss: 0.5874  time: 0.6337  data: 0.0000  max mem: 36248
Train: data epoch: [1] Total time: 0:02:07 (0.6390 s / it)
2025-05-31 01:43:22,772 [INFO] Averaged stats: lr: 0.0000  loss: 0.6638
2025-05-31 01:43:22,775 [INFO] No validation splits found.
2025-05-31 01:43:22,795 [INFO] Saving checkpoint at epoch 1 to /mnt/bst/hxu10/hxu10/Abdullahil-Oaphy/MiniGPT-4/outputs/20250531013/checkpoint_1.pth.
2025-05-31 01:43:22,883 [INFO] Start training
2025-05-31 01:43:22,902 [INFO] Start training epoch 2, 200 iters per inner epoch.
Train: data epoch: [2]  [  0/200]  eta: 0:01:53  lr: 0.000023  loss: 0.6978  time: 0.5684  data: 0.0000  max mem: 36248
Train: data epoch: [2]  [ 50/200]  eta: 0:01:40  lr: 0.000022  loss: 0.5878  time: 0.6238  data: 0.0000  max mem: 36248
Train: data epoch: [2]  [100/200]  eta: 0:01:04  lr: 0.000020  loss: 0.5881  time: 0.6125  data: 0.0000  max mem: 36248
Train: data epoch: [2]  [150/200]  eta: 0:00:31  lr: 0.000018  loss: 0.5699  time: 0.6156  data: 0.0000  max mem: 36248
Train: data epoch: [2]  [199/200]  eta: 0:00:00  lr: 0.000017  loss: 0.7084  time: 0.6205  data: 0.0000  max mem: 36248
Train: data epoch: [2] Total time: 0:02:08 (0.6443 s / it)
2025-05-31 01:45:31,773 [INFO] Averaged stats: lr: 0.0000  loss: 0.6491
2025-05-31 01:45:31,776 [INFO] No validation splits found.
2025-05-31 01:45:31,795 [INFO] Saving checkpoint at epoch 2 to /mnt/bst/hxu10/hxu10/Abdullahil-Oaphy/MiniGPT-4/outputs/20250531013/checkpoint_2.pth.
2025-05-31 01:45:31,877 [INFO] Start training
2025-05-31 01:45:31,896 [INFO] Start training epoch 3, 200 iters per inner epoch.
Train: data epoch: [3]  [  0/200]  eta: 0:02:06  lr: 0.000017  loss: 0.6647  time: 0.6319  data: 0.0000  max mem: 36248
Train: data epoch: [3]  [ 50/200]  eta: 0:01:34  lr: 0.000015  loss: 0.5597  time: 0.6247  data: 0.0000  max mem: 36248
Train: data epoch: [3]  [100/200]  eta: 0:01:02  lr: 0.000014  loss: 0.5980  time: 0.6214  data: 0.0000  max mem: 36248
Train: data epoch: [3]  [150/200]  eta: 0:00:32  lr: 0.000013  loss: 0.5609  time: 0.6291  data: 0.0000  max mem: 36248
Train: data epoch: [3]  [199/200]  eta: 0:00:00  lr: 0.000012  loss: 0.5348  time: 0.6323  data: 0.0000  max mem: 36248
Train: data epoch: [3] Total time: 0:02:07 (0.6384 s / it)
2025-05-31 01:47:39,568 [INFO] Averaged stats: lr: 0.0000  loss: 0.6366
2025-05-31 01:47:39,571 [INFO] No validation splits found.
2025-05-31 01:47:39,590 [INFO] Saving checkpoint at epoch 3 to /mnt/bst/hxu10/hxu10/Abdullahil-Oaphy/MiniGPT-4/outputs/20250531013/checkpoint_3.pth.
2025-05-31 01:47:39,672 [INFO] Start training
2025-05-31 01:47:39,693 [INFO] Start training epoch 4, 200 iters per inner epoch.
Train: data epoch: [4]  [  0/200]  eta: 0:02:05  lr: 0.000012  loss: 0.5089  time: 0.6270  data: 0.0000  max mem: 36248
Train: data epoch: [4]  [ 50/200]  eta: 0:01:33  lr: 0.000011  loss: 0.7710  time: 0.6151  data: 0.0000  max mem: 36248
Train: data epoch: [4]  [100/200]  eta: 0:01:04  lr: 0.000010  loss: 0.7045  time: 0.6306  data: 0.0000  max mem: 36248
Train: data epoch: [4]  [150/200]  eta: 0:00:31  lr: 0.000010  loss: 0.5174  time: 0.6227  data: 0.0000  max mem: 36248
Train: data epoch: [4]  [199/200]  eta: 0:00:00  lr: 0.000010  loss: 0.6389  time: 0.6244  data: 0.0000  max mem: 36248
Train: data epoch: [4] Total time: 0:02:06 (0.6341 s / it)
2025-05-31 01:49:46,514 [INFO] Averaged stats: lr: 0.0000  loss: 0.6193
2025-05-31 01:49:46,517 [INFO] No validation splits found.
2025-05-31 01:49:46,537 [INFO] Saving checkpoint at epoch 4 to /mnt/bst/hxu10/hxu10/Abdullahil-Oaphy/MiniGPT-4/outputs/20250531013/checkpoint_4.pth.
2025-05-31 01:49:46,617 [INFO] No validation splits found.
2025-05-31 01:49:46,617 [INFO] Training time 0:11:23
