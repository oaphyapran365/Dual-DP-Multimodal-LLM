model:
  arch: minigpt4
  model_type: pretrain_vicuna0
  llama_model: 
  ckpt: 
  image_size: 224
  max_txt_len: 160
  end_sym: "###"
  low_resource: False
  prompt_path: prompts/alignment.txt
  prompt_template: "###Human: {} ###Assistant:"
  q_former_model: ""
  num_query_token: 32
  encoder_config:
    hidden_size: 768

datasets:
  cc_sbu_align:
    vis_processor:
      eval:
        name: "blip2_image_eval"
        image_size: 224

run:
  task: image_text_pretrain
  device: cuda

